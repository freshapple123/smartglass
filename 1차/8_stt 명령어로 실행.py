import cv2
import numpy as np
from ultralytics import YOLO
import os
import time
import requests
import base64
import json
from dotenv import load_dotenv
from difflib import SequenceMatcher
import io
from PIL import Image
import speech_recognition as sr
import openai
from gtts import gTTS
import pygame

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
API_KEY = os.getenv("GOOGLE_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# OpenAI í´ë¼ì´ì–¸íŠ¸
client = openai.OpenAI(api_key=OPENAI_API_KEY)

# ì±… ì œëª©
book_title = "ë…¼ì–´"

# ìŒì„± ì¶œë ¥ í•¨ìˆ˜
def speak(text):
    tts = gTTS(text=text, lang="ko")
    fp = io.BytesIO()
    tts.write_to_fp(fp)
    fp.seek(0)
    pygame.mixer.init()
    pygame.mixer.music.load(fp)
    pygame.mixer.music.play()
    while pygame.mixer.music.get_busy():
        continue
    pygame.mixer.quit()

# ì´ë¯¸ì§€ ì¸ì½”ë”© í•¨ìˆ˜
def encode_image_cv2_to_base64(cv2_img):
    pil_img = Image.fromarray(cv2.cvtColor(cv2_img, cv2.COLOR_BGR2RGB))
    buffer = io.BytesIO()
    pil_img.save(buffer, format="JPEG")
    return base64.b64encode(buffer.getvalue()).decode("utf-8")

# OCR í•¨ìˆ˜
def detect_text_from_array(cv2_img):
    content = encode_image_cv2_to_base64(cv2_img)
    url = f"https://vision.googleapis.com/v1/images:annotate?key={API_KEY}"
    payload = {
        "requests": [{"image": {"content": content}, "features": [{"type": "TEXT_DETECTION"}]}]
    }
    headers = {"Content-Type": "application/json"}
    response = requests.post(url, headers=headers, data=json.dumps(payload))
    result = response.json()
    try:
        return result["responses"][0]["textAnnotations"][0]["description"].strip()
    except (KeyError, IndexError):
        return ""

# í˜ì´ì§€ ë¹„êµ í•¨ìˆ˜
def is_different_page(text1, text2, threshold=0.5):
    ratio = SequenceMatcher(None, text1, text2).ratio()
    return ratio < threshold

# ì±… OCR + ê·¸ë§Œ ê°ì§€ í†µí•© í•¨ìˆ˜
def auto_detect_and_ocr_with_stop(model_path="yolo11n.pt"):
    model = YOLO(model_path)
    cap = cv2.VideoCapture(1)
    if not cap.isOpened():
        print("âŒ ì›¹ìº  ì—´ê¸° ì‹¤íŒ¨")
        return

    recognizer = sr.Recognizer()
    mic = sr.Microphone()

    save_dir = f"book_pages/{book_title}"
    os.makedirs(save_dir, exist_ok=True)
    text_file_path = f"{book_title}.txt"
    image_counter = 1
    prev_left_text = ""
    prev_right_text = ""
    book_detected_time = None
    skip_until_time = 0

    print(f"ğŸ“˜ ì±…: {book_title} | í…ìŠ¤íŠ¸ ì €ì¥ ì‹œì‘ (ESC ë˜ëŠ” 'ê·¸ë§Œ'ìœ¼ë¡œ ì¢…ë£Œ)")

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # ìŒì„± ì¸ì‹ ì²´í¬
        with mic as source:
            recognizer.adjust_for_ambient_noise(source, duration=0.2)
            try:
                audio = recognizer.listen(source, timeout=1, phrase_time_limit=3)
                speech = recognizer.recognize_google(audio, language="ko-KR")
                if "ê·¸ë§Œ" in speech:
                    print("ğŸ›‘ 'ê·¸ë§Œ' ì¸ì‹ë¨. OCR ì¢…ë£Œ")
                    speak("ì±… ì½ê¸°ë¥¼ ì¢…ë£Œí• ê²Œìš”.")
                    break
            except (sr.WaitTimeoutError, sr.UnknownValueError):
                pass

        results = model(frame, verbose=False)[0]
        book_box = None
        for box in results.boxes:
            if int(box.cls[0]) == 73:
                book_box = box
                break

        if book_box:
            x1, y1, x2, y2 = map(int, book_box.xyxy[0])
            now = time.time()
            if book_detected_time is None:
                book_detected_time = now

            if now - book_detected_time >= 3 and now >= skip_until_time:
                book_crop = frame[y1:y2, x1:x2]
                h, w, _ = book_crop.shape
                mid = w // 2
                left_page = book_crop[:, :mid]
                right_page = book_crop[:, mid:]

                left_text = detect_text_from_array(left_page)
                right_text = detect_text_from_array(right_page)

                if len(left_text.strip()) < 30 and len(right_text.strip()) < 30:
                    print("âŒ ê¸€ì ìˆ˜ ë¶€ì¡± - ìƒëµ")
                    skip_until_time = now + 3
                    continue

                if is_different_page(left_text, prev_left_text) or is_different_page(right_text, prev_right_text):
                    with open(text_file_path, "a", encoding="utf-8") as f:
                        f.write(f"\n--- ì™¼ìª½ í˜ì´ì§€ ({image_counter}) ---\n{left_text}\n")
                        f.write(f"\n--- ì˜¤ë¥¸ìª½ í˜ì´ì§€ ({image_counter}) ---\n{right_text}\n")
                    print(f"âœ… ì €ì¥ ì™„ë£Œ: í˜ì´ì§€ {image_counter}")
                    prev_left_text = left_text
                    prev_right_text = right_text
                    image_counter += 1
                    book_detected_time = None
                    skip_until_time = 0
                else:
                    print("â© í˜ì´ì§€ ë™ì¼ - ì¬ì‹œë„ ëŒ€ê¸°")
                    skip_until_time = now + 3
        else:
            book_detected_time = None
            skip_until_time = 0

        cv2.imshow("Auto Book Scan", frame)
        if cv2.waitKey(1) & 0xFF == 27:
            print("ğŸ›‘ ESC ëˆ„ë¦„. ì¢…ë£Œ")
            break

    cap.release()
    cv2.destroyAllWindows()

# GPT ëŒ€í™” + ë©”ë‰´ ì²˜ë¦¬ (Rule-based)
def gpt_menu_conversation():
    recognizer = sr.Recognizer()
    while True:
        try:
            with sr.Microphone() as source:
                print("\nğŸ¤ ë©”ë‰´ë¥¼ ë§ì”€í•´ì£¼ì„¸ìš”: 'ì±…ì½ê¸°' ë˜ëŠ” 'ê·¸ë§Œ'")
                speak("ì±…ì½ê¸°, ë˜ëŠ” ê·¸ë§Œ ì¤‘ì—ì„œ ë§í•´ì£¼ì„¸ìš”.")
                recognizer.adjust_for_ambient_noise(source)
                audio = recognizer.listen(source)

            user_speech = recognizer.recognize_google(audio, language="ko-KR")
            print("ğŸ—£ï¸ ì¸ì‹ëœ ë§:", user_speech)

            if "ì±…ì½ê¸°" in user_speech:
                speak("ì±…ì„ ìŠ¤ìº”í• ê²Œìš”. ì¤‘ê°„ì— 'ê·¸ë§Œ'ì´ë¼ê³  ë§í•˜ë©´ ë©ˆì¶°ìš”.")
                auto_detect_and_ocr_with_stop()
                continue
            elif "ê·¸ë§Œ" in user_speech:
                speak("ì¢…ë£Œí• ê²Œìš”.")
                break
            else:
                speak("ì£„ì†¡í•´ìš”. 'ì±…ì½ê¸°'ë‚˜ 'ê·¸ë§Œ' ì¤‘ì—ì„œ ë§ì”€í•´ì£¼ì„¸ìš”.")
        except sr.UnknownValueError:
            print("âŒ ìŒì„± ì¸ì‹ ì‹¤íŒ¨")
            speak("ë‹¤ì‹œ ë§ì”€í•´ì£¼ì„¸ìš”.")
        except Exception as e:
            print("âš ï¸ ì˜¤ë¥˜ ë°œìƒ:", e)

# ì›¨ì´í¬ì›Œë“œ ê°ì§€ ë£¨í”„
def listen_for_wakeword():
    recognizer = sr.Recognizer()
    print("ğŸ¤ ì›¨ì´í¬ì›Œë“œ 'ë³µëŒ'ì„ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘...")

    while True:
        with sr.Microphone() as source:
            recognizer.adjust_for_ambient_noise(source)
            audio = recognizer.listen(source)

        try:
            text = recognizer.recognize_google(audio, language="ko-KR")
            print("ğŸ“ ì¸ì‹:", text)

            if "ë³µëŒ" in text:
                speak("ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
                gpt_menu_conversation()
        except:
            continue

# ì‹¤í–‰
listen_for_wakeword()
